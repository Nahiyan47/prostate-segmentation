#main_code_original
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import nibabel as nib
from tqdm import tqdm
import torch.optim as optim
import random
import warnings
import gc
import pickle
import cv2
from scipy import ndimage
import math
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.colors import ListedColormap
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.empty_cache()
print(f"Using device: {device}")

# NEW: Transition Zone Patch Extractor
class TransitionZonePatchExtractor:
    """Extract patches specifically around transition zones for detailed analysis"""
    
    def __init__(self, patch_size=96, overlap=0.5, min_transition_pixels=5):
        self.patch_size = patch_size
        self.overlap = overlap
        self.stride = int(patch_size * (1 - overlap))
        self.min_transition_pixels = min_transition_pixels
    
    def extract_transition_patches(self, image, label):
        """Extract patches that contain transition zone pixels"""
        patches = []
        patch_labels = []
        patch_coords = []
        
        h, w = image.shape[-2:]
        
        # Find all transition zone pixels
        transition_mask = (label == 1)
        
        if transition_mask.sum() == 0:
            # No transition zone, return center patch
            center_y, center_x = h // 2, w // 2
            patch_y = max(0, min(h - self.patch_size, center_y - self.patch_size // 2))
            patch_x = max(0, min(w - self.patch_size, center_x - self.patch_size // 2))
            
            patch_img = image[..., patch_y:patch_y + self.patch_size, patch_x:patch_x + self.patch_size]
            patch_lbl = label[patch_y:patch_y + self.patch_size, patch_x:patch_x + self.patch_size]
            
            return [patch_img], [patch_lbl], [(patch_y, patch_x)]
        
        # Get bounding box of transition zone
        transition_coords = torch.where(transition_mask)
        y_min, y_max = transition_coords[0].min().item(), transition_coords[0].max().item()
        x_min, x_max = transition_coords[1].min().item(), transition_coords[1].max().item()
        
        # Expand bounding box
        margin = self.patch_size // 4
        y_min = max(0, y_min - margin)
        y_max = min(h, y_max + margin)
        x_min = max(0, x_min - margin)
        x_max = min(w, x_max + margin)
        
        # Extract overlapping patches in transition region
        for y in range(y_min, y_max - self.patch_size + 1, self.stride):
            for x in range(x_min, x_max - self.patch_size + 1, self.stride):
                # Ensure patch fits
                y_end = min(y + self.patch_size, h)
                x_end = min(x + self.patch_size, w)
                y_start = y_end - self.patch_size
                x_start = x_end - self.patch_size
                
                if y_start < 0 or x_start < 0:
                    continue
                
                patch_lbl = label[y_start:y_end, x_start:x_end]
                
                # Only keep patches with transition pixels
                if torch.sum(patch_lbl == 1) >= self.min_transition_pixels:
                    patch_img = image[..., y_start:y_end, x_start:x_end]
                    patches.append(patch_img)
                    patch_labels.append(patch_lbl)
                    patch_coords.append((y_start, x_start))
        
        # If no patches found, extract center patch around largest transition cluster
        if len(patches) == 0:
            # Find center of mass of transition zone
            transition_y, transition_x = torch.where(transition_mask)
            center_y = int(transition_y.float().mean().item())
            center_x = int(transition_x.float().mean().item())
            
            patch_y = max(0, min(h - self.patch_size, center_y - self.patch_size // 2))
            patch_x = max(0, min(w - self.patch_size, center_x - self.patch_size // 2))
            
            patch_img = image[..., patch_y:patch_y + self.patch_size, patch_x:patch_x + self.patch_size]
            patch_lbl = label[patch_y:patch_y + self.patch_size, patch_x:patch_x + self.patch_size]
            
            patches.append(patch_img)
            patch_labels.append(patch_lbl)
            patch_coords.append((patch_y, patch_x))
        
        return patches, patch_labels, patch_coords

# Dataset with harmful slice removal and FIXED ROI extraction
class ProstateFocusedDataset(Dataset):
    def __init__(self, image_paths, label_paths, augment=False, use_patches=True):
        self.image_paths = image_paths
        self.label_paths = label_paths
        self.augment = augment
        self.use_patches = use_patches
        self.patch_extractor = TransitionZonePatchExtractor(patch_size=96, overlap=0.5)
        
        # Remove harmful slices
        self.filtered_paths = self.remove_harmful_slices()
        self.sample_weights = self.analyze_transition_content()
        
    def remove_harmful_slices(self):
        """Remove slices that hurt transition zone performance"""
        print("üîç Removing harmful slices...")
        
        good_image_paths = []
        good_label_paths = []
        removed = 0
        
        for image_path, label_path in zip(self.image_paths, self.label_paths):
            try:
                label_nii = nib.load(label_path)
                label = label_nii.get_fdata()
                
                if len(label.shape) == 4:
                    mid_slice = label.shape[2] // 2
                    label_slice = label[:, :, mid_slice, 0] if label.shape[3] == 1 else label[:, :, mid_slice]
                elif len(label.shape) == 3:
                    mid_slice = label.shape[2] // 2
                    label_slice = label[:, :, mid_slice]
                else:
                    label_slice = label
                
                transition_pixels = np.sum(label_slice == 1)
                total_prostate = np.sum(label_slice > 0)
                
                # Remove ONLY harmful slices
                should_remove = False
                
                if transition_pixels == 0:
                    should_remove = True
                elif transition_pixels <= 2:
                    should_remove = True
                elif total_prostate > 0 and (transition_pixels / total_prostate) > 0.7:
                    should_remove = True
                elif total_prostate == 0:
                    should_remove = True
                
                if should_remove:
                    removed += 1
                else:
                    good_image_paths.append(image_path)
                    good_label_paths.append(label_path)
                    
            except:
                removed += 1
        
        print(f"  Removed {removed} harmful slices, kept {len(good_image_paths)}")
        return good_image_paths, good_label_paths
        
    def analyze_transition_content(self):
        print("üîç Analyzing transition zone content...")
        weights = []
        
        image_paths, label_paths = self.filtered_paths
        
        for label_path in label_paths:
            try:
                label_nii = nib.load(label_path)
                label = label_nii.get_fdata()
                
                if len(label.shape) == 4:
                    mid_slice = label.shape[2] // 2
                    label_slice = label[:, :, mid_slice, 0] if label.shape[3] == 1 else label[:, :, mid_slice]
                elif len(label.shape) == 3:
                    mid_slice = label.shape[2] // 2
                    label_slice = label[:, :, mid_slice]
                else:
                    label_slice = label
                
                transition_pixels = np.sum(label_slice == 1)
                
                if transition_pixels > 20:
                    weight = 15.0
                else:
                    weight = 1.0
                
                weights.append(weight)
                
            except:
                weights.append(1.0)
        
        print(f"Weighted sampling setup complete")
        return weights
        
    def __len__(self):
        image_paths, _ = self.filtered_paths
        return len(image_paths)
    
    def get_complete_prostate_roi(self, image, label):
        """
        üéØ FIXED: Tighter crop that keeps prostate centered but removes excess background
        """
        prostate_mask = (label > 0).astype(np.uint8)
        
        if prostate_mask.sum() == 0:
            return image, label
        
        # Find bounding box of ALL prostate pixels
        coords = np.where(prostate_mask)
        y_min, y_max = coords[0].min(), coords[0].max()
        x_min, x_max = coords[1].min(), coords[1].max()
        
        # Calculate current prostate dimensions
        prostate_height = y_max - y_min + 1
        prostate_width = x_max - x_min + 1
        
        # üîß FIX: Use MUCH smaller margins - 30% instead of 80%
        # This gives context without too much background
        margin_y = int(prostate_height * 0.30)
        margin_x = int(prostate_width * 0.30)
        
        # Ensure minimum margin for small prostates
        margin_y = max(margin_y, 15)
        margin_x = max(margin_x, 15)
        
        # Calculate prostate center
        prostate_center_y = (y_min + y_max) // 2
        prostate_center_x = (x_min + x_max) // 2
        
        # Calculate desired crop size (prostate + margins, then make square)
        crop_height = prostate_height + 2 * margin_y
        crop_width = prostate_width + 2 * margin_x
        crop_size = max(crop_height, crop_width)
        
        # üîß FIX: Add a maximum crop size limit to avoid too much background
        h, w = image.shape
        max_crop_size = min(int(min(h, w) * 0.6), crop_size)  # Max 60% of image size
        crop_size = min(crop_size, max_crop_size)
        
        half_size = crop_size // 2
        
        # Calculate square boundaries centered on prostate
        square_y_min = prostate_center_y - half_size
        square_y_max = prostate_center_y + half_size
        square_x_min = prostate_center_x - half_size
        square_x_max = prostate_center_x + half_size
        
        # Adjust for image boundaries while keeping prostate centered
        if square_y_min < 0:
            shift = -square_y_min
            square_y_min = 0
            square_y_max = min(h, square_y_max + shift)
        elif square_y_max > h:
            shift = square_y_max - h
            square_y_max = h
            square_y_min = max(0, square_y_min - shift)
        
        if square_x_min < 0:
            shift = -square_x_min
            square_x_min = 0
            square_x_max = min(w, square_x_max + shift)
        elif square_x_max > w:
            shift = square_x_max - w
            square_x_max = w
            square_x_min = max(0, square_x_min - shift)
        
        # Extract ROI
        roi_image = image[square_y_min:square_y_max, square_x_min:square_x_max]
        roi_label = label[square_y_min:square_y_max, square_x_min:square_x_max]
        
        # Verify we didn't lose prostate pixels
        original_prostate = np.sum(label > 0)
        roi_prostate = np.sum(roi_label > 0)
        
        if roi_prostate < original_prostate * 0.98:  # Lost more than 2%
            print(f"‚ö†Ô∏è Lost {original_prostate - roi_prostate} prostate pixels!")
            
            # Fallback: Use prostate bounding box + 40% margin
            fallback_margin_y = int(prostate_height * 0.40)
            fallback_margin_x = int(prostate_width * 0.40)
            
            fb_y_min = max(0, y_min - fallback_margin_y)
            fb_y_max = min(h, y_max + fallback_margin_y)
            fb_x_min = max(0, x_min - fallback_margin_x)
            fb_x_max = min(w, x_max + fallback_margin_x)
            
            # Make square centered on prostate
            fb_height = fb_y_max - fb_y_min
            fb_width = fb_x_max - fb_x_min
            fb_size = max(fb_height, fb_width)
            
            fb_half = fb_size // 2
            fb_y_min = max(0, prostate_center_y - fb_half)
            fb_y_max = min(h, prostate_center_y + fb_half)
            fb_x_min = max(0, prostate_center_x - fb_half)
            fb_x_max = min(w, prostate_center_x + fb_half)
            
            roi_image = image[fb_y_min:fb_y_max, fb_x_min:fb_x_max]
            roi_label = label[fb_y_min:fb_y_max, fb_x_min:fb_x_max]
        
        return roi_image, roi_label
    
    def __getitem__(self, idx):
        # Use filtered paths
        image_paths, label_paths = self.filtered_paths
        
        img_nii = nib.load(image_paths[idx])
        label_nii = nib.load(label_paths[idx])
        
        image = img_nii.get_fdata()
        label = label_nii.get_fdata()
        
        if len(image.shape) == 4:
            mid_slice = image.shape[2] // 2
            image = image[:, :, mid_slice, 0]
            label = label[:, :, mid_slice] if len(label.shape) == 3 else label[:, :, mid_slice, 0]
        elif len(image.shape) == 3:
            mid_slice = image.shape[2] // 2
            image = image[:, :, mid_slice]
            label = label[:, :, mid_slice]
        
        if len(image.shape) > 2:
            image = np.squeeze(image)
        if len(label.shape) > 2:
            label = np.squeeze(label)
        
        # üîß FIX 1: Extract ROI FIRST (before any resizing)
        image, label = self.get_complete_prostate_roi(image, label)
        
        # Normalize based on prostate region
        prostate_mask = label > 0
        if prostate_mask.sum() > 0:
            prostate_pixels = image[prostate_mask]
            p1, p99 = np.percentile(prostate_pixels, [1, 99])
            image = np.clip(image, p1, p99)
        else:
            image = np.clip(image, np.percentile(image, 1), np.percentile(image, 99))
        
        image_min, image_max = image.min(), image.max()
        if image_max > image_min:
            image = (image - image_min) / (image_max - image_min)
        else:
            image = np.zeros_like(image)
        
        label = label.astype(np.int64)
        
        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)
        label = torch.tensor(label, dtype=torch.long)
        
        # üîß FIX 2: Resize DIRECTLY to 96x96 (not 192 then 96)
        # This preserves the centering from ROI extraction
        target_size = 96
        image = F.interpolate(image.unsqueeze(0), size=(target_size, target_size), mode='bilinear', align_corners=False).squeeze(0)
        label = F.interpolate(label.unsqueeze(0).unsqueeze(0).float(), size=(target_size, target_size), mode='nearest').squeeze(0).squeeze(0).long()
        
        # üîß FIX 3: Augmentation on FINAL size
        if self.augment and np.random.random() > 0.2:
            # Check for transition zone with CORRECT label value
            transition_pixels = torch.sum(label == 1).item()
            has_transition = transition_pixels > 10
            
            if has_transition:
                if np.random.random() > 0.4:
                    angle = np.random.uniform(-12, 12)
                    image = self.rotate_tensor(image, angle)
                    label = self.rotate_tensor(label.unsqueeze(0).float(), angle).squeeze(0).long()
                
                if np.random.random() > 0.5:
                    image, label = self.elastic_transform(image, label)
                
                if np.random.random() > 0.3:
                    gamma = np.random.uniform(0.8, 1.2)
                    image = torch.pow(image, gamma)
                    
                    contrast = np.random.uniform(0.9, 1.1)
                    image = torch.clamp(image * contrast, 0, 1)
            
            if np.random.random() > 0.5:
                image = torch.flip(image, [2])
                label = torch.flip(label, [1])
            
            if np.random.random() > 0.5:
                image = torch.flip(image, [1])
                label = torch.flip(label, [0])
        
        # üîß FIX 4: Remove the patch processing that causes off-centering
        # Just return the centered 96x96 image directly
        return image, label
    
    def process_with_patches(self, image, label):
        """Process image using transition-focused patches"""
        patches, patch_labels, patch_coords = self.patch_extractor.extract_transition_patches(image, label)
        
        if len(patches) > 1 and self.augment:
            transition_counts = [torch.sum(pl == 1).item() for pl in patch_labels]
            if max(transition_counts) > 0:
                weights = np.array(transition_counts) + 1
                weights = weights / weights.sum()
                selected_idx = np.random.choice(len(patches), p=weights)
            else:
                selected_idx = np.random.choice(len(patches))
            
            selected_patch = patches[selected_idx]
            selected_label = patch_labels[selected_idx]
        else:
            selected_patch = patches[0]
            selected_label = patch_labels[0]
        
        if selected_patch.shape[-1] != 96 or selected_patch.shape[-2] != 96:
            selected_patch = F.interpolate(selected_patch.unsqueeze(0), size=(96, 96), mode='bilinear', align_corners=False).squeeze(0)
            selected_label = F.interpolate(selected_label.unsqueeze(0).unsqueeze(0).float(), size=(96, 96), mode='nearest').squeeze(0).squeeze(0).long()
        
        return selected_patch, selected_label
    
    def elastic_transform(self, image, label, alpha=20, sigma=3):
        image_np = image.squeeze().numpy()
        label_np = label.numpy()
        
        shape = image_np.shape
        dx = ndimage.gaussian_filter(np.random.randn(*shape), sigma) * alpha
        dy = ndimage.gaussian_filter(np.random.randn(*shape), sigma) * alpha
        
        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
        x_new = np.clip(x + dx, 0, shape[1]-1)
        y_new = np.clip(y + dy, 0, shape[0]-1)
        
        image_def = ndimage.map_coordinates(image_np, [y_new, x_new], order=1, mode='reflect')
        label_def = ndimage.map_coordinates(label_np, [y_new, x_new], order=0, mode='reflect')
        
        return torch.tensor(image_def, dtype=torch.float32).unsqueeze(0), torch.tensor(label_def, dtype=torch.long)
    
    def rotate_tensor(self, tensor, angle):
        theta = torch.tensor([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0],
                             [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0]], dtype=torch.float32)
        grid = F.affine_grid(theta.unsqueeze(0), tensor.unsqueeze(0).size(), align_corners=False)
        return F.grid_sample(tensor.unsqueeze(0), grid, align_corners=False).squeeze(0)

# Simple SwinUNet implementation with attention capture
class SwinUNet(nn.Module):
    def __init__(self, in_channels=1, num_classes=3, img_size=96):
        super(SwinUNet, self).__init__()
        
        # Encoder
        self.encoder1 = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        
        self.encoder2 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.encoder3 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        self.encoder4 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        # Bottleneck with Attention
        self.bottleneck = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention for bottleneck
        self.attention = nn.MultiheadAttention(512, 8, batch_first=True, dropout=0.1)
        self.norm1 = nn.LayerNorm(512)
        self.norm2 = nn.LayerNorm(512)
        self.mlp = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),  # ADD THIS LINE
            nn.Linear(1024, 512)
        )
        self.attention_scale = nn.Parameter(torch.ones(1) * 5.0)  # Very strong - 5x
        
        # Store attention weights for visualization
        self.attention_weights = None
        
        # Decoder
        self.decoder4 = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 2, stride=2),
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        self.decoder3 = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 2, stride=2),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        self.decoder2 = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.decoder1 = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 2, stride=2),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        
        # Output layers
        self.final = nn.Conv2d(32, num_classes, 1)
        
        # Deep supervision
        self.aux_out1 = nn.Conv2d(256, num_classes, 1)
        self.aux_out2 = nn.Conv2d(128, num_classes, 1)
        
    def forward(self, x):
        # Encoder
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(enc1)
        enc3 = self.encoder3(enc2)
        enc4 = self.encoder4(enc3)
        
        # Bottleneck with attention
        bottleneck = self.bottleneck(enc4)
        
        # Apply self-attention with learnable scaling
        B, C, H, W = bottleneck.shape
        bottleneck_flat = bottleneck.view(B, C, H*W).permute(0, 2, 1)  # [B, H*W, C]

        # Self-attention with attention weights capture
        attn_out, attn_weights = self.attention(bottleneck_flat, bottleneck_flat, bottleneck_flat)

        # Store attention weights for visualization
        self.attention_weights = attn_weights

        # Apply attention with stronger influence using learnable scale
        attn_out = self.norm1(bottleneck_flat + self.attention_scale * attn_out)

        # MLP
        mlp_out = self.mlp(attn_out)
        mlp_out = self.norm2(attn_out + mlp_out)

        # Reshape back
        bottleneck = mlp_out.permute(0, 2, 1).view(B, C, H, W)
        
        # Decoder
        dec4 = self.decoder4[0](bottleneck)  # Upsample
        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection
        dec4 = self.decoder4[1:](dec4)  # Conv layers
        
        dec3 = self.decoder3[0](dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.decoder3[1:](dec3)
        
        dec2 = self.decoder2[0](dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.decoder2[1:](dec2)
        
        dec1 = self.decoder1[0](dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.decoder1[1:](dec1)
        
        # Final output
        out = self.final(dec1)
        
        # Auxiliary outputs
        aux1 = F.interpolate(self.aux_out1(dec4), size=out.shape[2:], mode='bilinear', align_corners=False)
        aux2 = F.interpolate(self.aux_out2(dec3), size=out.shape[2:], mode='bilinear', align_corners=False)
        
        return out, aux1, aux2

# NEW: Volume Calibrator Class
class VolumeCalibrator:
    """
    Post-processing volume calibration that preserves segmentation quality
    while fixing volume bias for clinical accuracy
    """
    
    def __init__(self):
        # Calibration factors learned from training data
        self.tz_bias_factor = 1.22  # Correct TZ underestimation (~18% bias)
        self.pz_bias_factor = 0.97  # Correct PZ slight overestimation (~3% bias)
        self.min_tz_pixels = 5
        self.min_pz_pixels = 10
        
    def calibrate_volumes(self, prediction, preserve_boundaries=True):
        """
        Calibrate predicted volumes while preserving segmentation boundaries
        """
        calibrated_pred = prediction.copy()
        
        # Get current volumes
        tz_mask = (prediction == 1)
        pz_mask = (prediction == 2)
        
        current_tz_volume = np.sum(tz_mask)
        current_pz_volume = np.sum(pz_mask)
        
        if current_tz_volume < self.min_tz_pixels and current_pz_volume < self.min_pz_pixels:
            return calibrated_pred  # Skip if no meaningful structures
        
        # Calculate target volumes
        target_tz_volume = int(current_tz_volume * self.tz_bias_factor)
        target_pz_volume = int(current_pz_volume * self.pz_bias_factor)
        
        # Apply volume corrections
        if preserve_boundaries:
            calibrated_pred = self._boundary_preserving_calibration(
                prediction, tz_mask, pz_mask, target_tz_volume, target_pz_volume
            )
        
        return calibrated_pred
    
    def _boundary_preserving_calibration(self, prediction, tz_mask, pz_mask, target_tz, target_pz):
        """
        Calibrate volumes while preserving original boundary shapes
        """
        calibrated = prediction.copy()
        
        # For TZ: Need to ADD pixels (underestimated)
        if target_tz > np.sum(tz_mask):
            pixels_to_add = target_tz - np.sum(tz_mask)
            calibrated = self._expand_region(calibrated, tz_mask, pixels_to_add, target_class=1)
        
        # For PZ: Need to REMOVE pixels (overestimated)
        if target_pz < np.sum(pz_mask):
            pixels_to_remove = np.sum(pz_mask) - target_pz
            calibrated = self._contract_region(calibrated, pz_mask, pixels_to_remove, target_class=2)
        
        return calibrated
    
    def _expand_region(self, prediction, region_mask, pixels_to_add, target_class):
        """Expand region by adding pixels at boundaries"""
        if pixels_to_add <= 0:
            return prediction
        
        # Find boundary pixels (adjacent to target region)
        from scipy import ndimage
        
        # Dilate the region to find expansion candidates
        expanded_mask = ndimage.binary_dilation(region_mask, iterations=2)
        boundary_candidates = expanded_mask & (~region_mask) & (prediction == 0)  # Only background pixels
        
        if np.sum(boundary_candidates) == 0:
            return prediction  # No expansion possible
        
        # Get boundary pixel coordinates
        boundary_coords = np.where(boundary_candidates)
        boundary_pixels = list(zip(boundary_coords[0], boundary_coords[1]))
        
        # Add pixels closest to existing region center
        if len(boundary_pixels) > pixels_to_add:
            # Calculate distances to region centroid
            region_coords = np.where(region_mask)
            if len(region_coords[0]) > 0:
                centroid_y = np.mean(region_coords[0])
                centroid_x = np.mean(region_coords[1])
                
                distances = [np.sqrt((y - centroid_y)**2 + (x - centroid_x)**2) 
                           for y, x in boundary_pixels]
                
                # Select closest pixels
                sorted_indices = np.argsort(distances)
                selected_pixels = [boundary_pixels[i] for i in sorted_indices[:pixels_to_add]]
            else:
                selected_pixels = boundary_pixels[:pixels_to_add]
        else:
            selected_pixels = boundary_pixels
        
        # Add selected pixels
        result = prediction.copy()
        for y, x in selected_pixels:
            result[y, x] = target_class
        
        return result
    
    def _contract_region(self, prediction, region_mask, pixels_to_remove, target_class):
        """Contract region by removing pixels at boundaries"""
        if pixels_to_remove <= 0:
            return prediction
        
        # Find boundary pixels (on the edge of the region)
        from scipy import ndimage
        
        # Erode the region to find contraction candidates
        eroded_mask = ndimage.binary_erosion(region_mask, iterations=1)
        boundary_candidates = region_mask & (~eroded_mask)
        
        if np.sum(boundary_candidates) == 0:
            return prediction  # No contraction possible
        
        # Get boundary pixel coordinates
        boundary_coords = np.where(boundary_candidates)
        boundary_pixels = list(zip(boundary_coords[0], boundary_coords[1]))
        
        # Remove pixels furthest from region center
        if len(boundary_pixels) > pixels_to_remove:
            # Calculate distances to region centroid
            region_coords = np.where(region_mask)
            if len(region_coords[0]) > 0:
                centroid_y = np.mean(region_coords[0])
                centroid_x = np.mean(region_coords[1])
                
                distances = [np.sqrt((y - centroid_y)**2 + (x - centroid_x)**2) 
                           for y, x in boundary_pixels]
                
                # Select furthest pixels
                sorted_indices = np.argsort(distances)[::-1]
                selected_pixels = [boundary_pixels[i] for i in sorted_indices[:pixels_to_remove]]
            else:
                selected_pixels = boundary_pixels[:pixels_to_remove]
        else:
            selected_pixels = boundary_pixels
        
        # Remove selected pixels
        result = prediction.copy()
        for y, x in selected_pixels:
            result[y, x] = 0  # Convert to background
        
        return result

# Loss function
class BCEDiceLoss(nn.Module):
    def __init__(self, weight_bce=0.5, weight_dice=0.5):
        super(BCEDiceLoss, self).__init__()
        self.weight_bce = weight_bce
        self.weight_dice = weight_dice
        self.bce = nn.CrossEntropyLoss()
        
    def dice_loss(self, pred, target, smooth=1e-7):
        pred = F.softmax(pred, dim=1)
        
        dice_scores = []
        for class_idx in [1, 2]:  # Only transition and peripheral
            pred_class = pred[:, class_idx, :, :]
            target_class = (target == class_idx).float()
            
            intersection = (pred_class * target_class).sum(dim=(1, 2))
            union = pred_class.sum(dim=(1, 2)) + target_class.sum(dim=(1, 2))
            
            dice = (2 * intersection + smooth) / (union + smooth)
            dice_scores.append(dice.mean())
        
        # Higher weight for transition zone
        weighted_dice = (dice_scores[0] * 5 + dice_scores[1] * 1) / 6
        return 1 - weighted_dice
    
    def forward(self, pred, target, aux_pred1=None, aux_pred2=None):
        # Main losses
        bce_loss = self.bce(pred, target)
        dice_loss = self.dice_loss(pred, target)
        
        total_loss = self.weight_bce * bce_loss + self.weight_dice * dice_loss
        
        # Deep supervision
        if aux_pred1 is not None:
            aux_bce1 = self.bce(aux_pred1, target)
            aux_dice1 = self.dice_loss(aux_pred1, target)
            total_loss += 0.3 * (self.weight_bce * aux_bce1 + self.weight_dice * aux_dice1)
        
        if aux_pred2 is not None:
            aux_bce2 = self.bce(aux_pred2, target)
            aux_dice2 = self.dice_loss(aux_pred2, target)
            total_loss += 0.2 * (self.weight_bce * aux_bce2 + self.weight_dice * aux_dice2)
        
        return total_loss

def create_weighted_sampler(dataset):
    weights = dataset.sample_weights
    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)
    return sampler

def prostate_dice_coefficient(pred, target):
    pred = F.softmax(pred, dim=1)
    dice_scores = []
    
    for class_idx in [1, 2]:
        pred_class = pred[:, class_idx, :, :]
        target_class = (target == class_idx).float()
        
        intersection = (pred_class * target_class).sum()
        union = pred_class.sum() + target_class.sum()
        
        if union == 0:
            dice = 0.0
        else:
            dice = (2 * intersection) / union
        
        dice_scores.append(dice.item())
    
    return dice_scores

def train_val_split(image_paths, label_paths, val_ratio=0.2, random_seed=42):
    random.seed(random_seed)
    np.random.seed(random_seed)
    
    indices = list(range(len(image_paths)))
    random.shuffle(indices)
    
    val_size = int(len(indices) * val_ratio)
    val_indices = indices[:val_size]
    train_indices = indices[val_size:]
    
    train_images = [image_paths[i] for i in train_indices]
    val_images = [image_paths[i] for i in val_indices]
    train_labels = [label_paths[i] for i in train_indices]
    val_labels = [label_paths[i] for i in val_indices]
    
    return train_images, val_images, train_labels, val_labels

def get_data_paths(data_dir, data_dir2):
    images_tr_dir = os.path.join(data_dir2, 'imagesTr_augmented')
    images_ts_dir = os.path.join(data_dir, 'imagesTs') 
    labels_tr_dir = os.path.join(data_dir2, 'labelsTr_augmented')
    
    image_files = [f for f in os.listdir(images_tr_dir) if f.endswith('.nii.gz') and not f.startswith('._')]
    label_files = [f for f in os.listdir(labels_tr_dir) if f.endswith('.nii.gz') and not f.startswith('._')]
    
    image_files.sort()
    label_files.sort()
    
    image_paths = [os.path.join(images_tr_dir, f) for f in image_files]
    label_paths = [os.path.join(labels_tr_dir, f) for f in label_files]
    
    test_image_files = []
    if os.path.exists(images_ts_dir):
        test_image_files = [f for f in os.listdir(images_ts_dir) if f.endswith('.nii.gz') and not f.startswith('._')]
        test_image_files.sort()
    
    test_image_paths = [os.path.join(images_ts_dir, f) for f in test_image_files]
    
    return image_paths, label_paths, test_image_paths

def safe_save_model(model, filepath):
    import sys
    try:
        if sys.version_info >= (3, 13):
            state_dict = {k: v.detach().cpu() for k, v in model.state_dict().items()}
            with open(filepath, 'wb') as f:
                pickle.dump(state_dict, f, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            torch.save(model.state_dict(), filepath)
        print(f"Model saved to {filepath}")
    except Exception as e:
        print(f"Save failed: {e}")

# NEW: Qualitative Results Visualization Functions
def create_qualitative_results_grid(model, val_loader, device, save_dir='results', num_samples=12):
    """
    Create a qualitative results grid showing original images, ground truth, and predictions
    """
    model.eval()
    
    # Create results directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Custom colormap for segmentation (background=black, transition=red, peripheral=blue)
    colors = ['black', 'red', 'blue']
    cmap = ListedColormap(colors)
    
    # Collect samples
    samples_collected = 0
    all_images = []
    all_ground_truth = []
    all_predictions = []
    all_dice_scores = []
    
    print(f"üé® Creating qualitative results grid with {num_samples} samples...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(val_loader):
            if samples_collected >= num_samples:
                break
                
            data, target = data.to(device), target.to(device)
            main_out, _, _ = model(data)
            
            # Get predictions
            pred_probs = F.softmax(main_out, dim=1)
            predictions = torch.argmax(pred_probs, dim=1)
            
            # Calculate Dice scores for this batch
            dice_scores = prostate_dice_coefficient(main_out, target)
            
            # Process each sample in the batch
            for i in range(data.shape[0]):
                if samples_collected >= num_samples:
                    break
                
                # Convert to numpy
                image = data[i, 0].cpu().numpy()  # Remove channel dimension
                gt = target[i].cpu().numpy()
                pred = predictions[i].cpu().numpy()
                
                # Only include samples with transition zone
                if np.sum(gt == 1) > 5:  # At least 5 transition pixels
                    all_images.append(image)
                    all_ground_truth.append(gt)
                    all_predictions.append(pred)
                    all_dice_scores.append((dice_scores[0], dice_scores[1]))
                    samples_collected += 1
    
    if len(all_images) == 0:
        print("‚ùå No suitable samples found for visualization")
        return
    
    # Create the grid
    n_cols = 3  # Original, Ground Truth, Prediction
    n_rows = min(len(all_images), num_samples)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    fig.suptitle('Qualitative Segmentation Results: SwinUNet with Bounding Box Cropping', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    
    for row in range(n_rows):
        # Original Image
        axes[row, 0].imshow(all_images[row], cmap='gray')
        axes[row, 0].set_title(f'Original Image #{row+1}', fontweight='bold')
        axes[row, 0].axis('off')
        
        # Ground Truth
        axes[row, 1].imshow(all_images[row], cmap='gray', alpha=0.7)
        gt_overlay = np.ma.masked_where(all_ground_truth[row] == 0, all_ground_truth[row])
        axes[row, 1].imshow(gt_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        axes[row, 1].set_title('Ground Truth', fontweight='bold')
        axes[row, 1].axis('off')
        
        # Add legend for ground truth
        if row == 0:
            legend_elements = [
                patches.Patch(color='red', label='Peripheral Zone'),
                patches.Patch(color='blue', label='Transition Zone')
            ]
            axes[row, 1].legend(handles=legend_elements, loc='upper right', fontsize=8)
        
        # Prediction
        axes[row, 2].imshow(all_images[row], cmap='gray', alpha=0.7)
        pred_overlay = np.ma.masked_where(all_predictions[row] == 0, all_predictions[row])
        axes[row, 2].imshow(pred_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        
        # Add Dice scores to prediction title
        t_dice, p_dice = all_dice_scores[row]
        axes[row, 2].set_title(f'Prediction\nP-Dice: {t_dice:.3f}, T-Dice: {p_dice:.3f}', 
                              fontweight='bold', fontsize=10)
        axes[row, 2].axis('off')
        
        # Add sample statistics
        gt_transition = np.sum(all_ground_truth[row] == 1)
        gt_peripheral = np.sum(all_ground_truth[row] == 2)
        pred_transition = np.sum(all_predictions[row] == 1)
        pred_peripheral = np.sum(all_predictions[row] == 2)
        
        # Add text with pixel counts
        stats_text = f'GT: T={gt_transition}, T={gt_peripheral}\nPred: P={pred_transition}, T={pred_peripheral}'
        axes[row, 0].text(0.02, 0.98, stats_text, transform=axes[row, 0].transAxes, 
                         verticalalignment='top', fontsize=8, 
                         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    
    # Save the grid
    grid_path = os.path.join(save_dir, 'qualitative_results_grid.png')
    plt.savefig(grid_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.savefig(os.path.join(save_dir, 'qualitative_results_grid.pdf'), 
                bbox_inches='tight', facecolor='white')
    
    print(f"‚úÖ Qualitative results grid saved to {grid_path}")
    
    # Create a separate figure for best and worst cases
    create_best_worst_cases(all_images, all_ground_truth, all_predictions, all_dice_scores, save_dir)
    
    plt.show()
    
    return fig

def create_best_worst_cases(images, ground_truths, predictions, dice_scores, save_dir):
    """Create visualization of best and worst segmentation cases"""
    
    # Calculate average dice for sorting
    avg_dice_scores = [(t_dice + p_dice) / 2 for t_dice, p_dice in dice_scores]
    
    # Get indices for best and worst cases
    sorted_indices = sorted(range(len(avg_dice_scores)), key=lambda k: avg_dice_scores[k])
    worst_indices = sorted_indices[:3]  # 3 worst cases
    best_indices = sorted_indices[-3:]  # 3 best cases
    
    # Custom colormap
    colors = ['black', 'red', 'blue']
    cmap = ListedColormap(colors)
    
    fig, axes = plt.subplots(2, 9, figsize=(20, 8))
    fig.suptitle('Best vs Worst Segmentation Cases', fontsize=16, fontweight='bold')
    
    # Plot worst cases
    for i, idx in enumerate(worst_indices):
        # Original
        axes[0, i*3].imshow(images[idx], cmap='gray')
        axes[0, i*3].set_title(f'Worst #{i+1}\nOriginal', fontsize=10)
        axes[0, i*3].axis('off')
        
        # Ground Truth
        axes[0, i*3+1].imshow(images[idx], cmap='gray', alpha=0.7)
        gt_overlay = np.ma.masked_where(ground_truths[idx] == 0, ground_truths[idx])
        axes[0, i*3+1].imshow(gt_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        axes[0, i*3+1].set_title('Ground Truth', fontsize=10)
        axes[0, i*3+1].axis('off')
        
        # Prediction
        axes[0, i*3+2].imshow(images[idx], cmap='gray', alpha=0.7)
        pred_overlay = np.ma.masked_where(predictions[idx] == 0, predictions[idx])
        axes[0, i*3+2].imshow(pred_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        t_dice, p_dice = dice_scores[idx]
        axes[0, i*3+2].set_title(f'Prediction\nP:{t_dice:.3f} T:{p_dice:.3f}', fontsize=10)
        axes[0, i*3+2].axis('off')
    
    # Plot best cases
    for i, idx in enumerate(best_indices):
        # Original
        axes[1, i*3].imshow(images[idx], cmap='gray')
        axes[1, i*3].set_title(f'Best #{i+1}\nOriginal', fontsize=10)
        axes[1, i*3].axis('off')
        
        # Ground Truth
        axes[1, i*3+1].imshow(images[idx], cmap='gray', alpha=0.7)
        gt_overlay = np.ma.masked_where(ground_truths[idx] == 0, ground_truths[idx])
        axes[1, i*3+1].imshow(gt_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        axes[1, i*3+1].set_title('Ground Truth', fontsize=10)
        axes[1, i*3+1].axis('off')
        
        # Prediction
        axes[1, i*3+2].imshow(images[idx], cmap='gray', alpha=0.7)
        pred_overlay = np.ma.masked_where(predictions[idx] == 0, predictions[idx])
        axes[1, i*3+2].imshow(pred_overlay, cmap=cmap, alpha=0.8, vmin=0, vmax=2)
        t_dice, p_dice = dice_scores[idx]
        axes[1, i*3+2].set_title(f'Prediction\nT:{t_dice:.3f} P:{p_dice:.3f}', fontsize=10)
        axes[1, i*3+2].axis('off')
    
    plt.tight_layout()
    
    # Save best/worst cases
    best_worst_path = os.path.join(save_dir, 'best_worst_cases.png')
    plt.savefig(best_worst_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.savefig(os.path.join(save_dir, 'best_worst_cases.pdf'), 
                bbox_inches='tight', facecolor='white')
    
    print(f"‚úÖ Best/worst cases saved to {best_worst_path}")
    plt.show()

# NEW: Attention Maps Visualization Functions
def create_attention_maps(model, val_loader, device, save_dir='results', num_samples=8):
    """
    Create attention maps visualization showing where the self-attention mechanism focuses
    """
    model.eval()
    
    # Create results directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Custom colormap for segmentation
    colors = ['black', 'red', 'blue']
    seg_cmap = ListedColormap(colors)
    
    # Collect samples with attention maps
    samples_collected = 0
    all_data = []
    
    print(f"üß† Creating attention maps visualization with {num_samples} samples...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(val_loader):
            if samples_collected >= num_samples:
                break
                
            data, target = data.to(device), target.to(device)
            
            # Forward pass to get predictions and attention weights
            main_out, _, _ = model(data)
            attention_weights = model.attention_weights  # [batch_size, seq_len, seq_len]
            
            # Get predictions
            predictions = torch.argmax(F.softmax(main_out, dim=1), dim=1)
            
            # Process each sample in the batch
            for i in range(data.shape[0]):
                if samples_collected >= num_samples:
                    break
                
                # Convert to numpy
                image = data[i, 0].cpu().numpy()
                gt = target[i].cpu().numpy()
                pred = predictions[i].cpu().numpy()
                
                # Extract attention weights for this sample
                if attention_weights is not None and i < attention_weights.shape[0]:
                    # Average across attention heads and get attention map
                    attn_map = attention_weights[i].cpu().numpy()  # [seq_len, seq_len]
                    
                    # Get attention from first token (or average)
                    attn_avg = np.mean(attn_map, axis=0)  # Average attention across all positions
                    
                    # Determine spatial dimensions dynamically
                    seq_len = attn_avg.shape[0]
                    spatial_size = int(np.sqrt(seq_len))
                    
                    if spatial_size * spatial_size == seq_len:
                        # Perfect square - reshape to spatial dimensions
                        attn_spatial = attn_avg.reshape(spatial_size, spatial_size)
                    else:
                        # Not a perfect square - pad or crop to nearest square
                        nearest_square = int(np.sqrt(seq_len))
                        if nearest_square * nearest_square < seq_len:
                            nearest_square += 1
                        
                        # Pad with zeros if needed
                        padded_size = nearest_square * nearest_square
                        attn_padded = np.zeros(padded_size)
                        attn_padded[:seq_len] = attn_avg
                        attn_spatial = attn_padded.reshape(nearest_square, nearest_square)
                    
                    # Upsample attention map to match image size (96x96)
                    attn_upsampled = cv2.resize(attn_spatial, (96, 96), interpolation=cv2.INTER_CUBIC)
                    
                    # Only include samples with transition zone
                    if np.sum(gt == 1) > 5:
                        all_data.append({
                            'image': image,
                            'ground_truth': gt,
                            'prediction': pred,
                            'attention_map': attn_upsampled
                        })
                        samples_collected += 1
    
    if len(all_data) == 0:
        print("‚ùå No suitable samples found for attention visualization")
        return
    
    # Create the attention visualization
    n_cols = 4  # Original, Ground Truth, Prediction, Attention Map
    n_rows = min(len(all_data), num_samples)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))
    fig.suptitle('Attention Maps: Where SwinUNet Focuses During Segmentation', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    
    for row in range(n_rows):
        data_item = all_data[row]
        
        # Original Image
        axes[row, 0].imshow(data_item['image'], cmap='gray')
        axes[row, 0].set_title(f'Original #{row+1}', fontweight='bold', fontsize=10)
        axes[row, 0].axis('off')
        
        # Ground Truth
        axes[row, 1].imshow(data_item['image'], cmap='gray', alpha=0.7)
        gt_overlay = np.ma.masked_where(data_item['ground_truth'] == 0, data_item['ground_truth'])
        axes[row, 1].imshow(gt_overlay, cmap=seg_cmap, alpha=0.8, vmin=0, vmax=2)
        axes[row, 1].set_title('Ground Truth', fontweight='bold', fontsize=10)
        axes[row, 1].axis('off')
        
        # Add legend for first row
        if row == 0:
            legend_elements = [
                patches.Patch(color='red', label='peripheral Zone'),
                patches.Patch(color='blue', label='Transition Zone')
            ]
            axes[row, 1].legend(handles=legend_elements, loc='upper right', fontsize=7)
        
        # Prediction
        axes[row, 2].imshow(data_item['image'], cmap='gray', alpha=0.7)
        pred_overlay = np.ma.masked_where(data_item['prediction'] == 0, data_item['prediction'])
        axes[row, 2].imshow(pred_overlay, cmap=seg_cmap, alpha=0.8, vmin=0, vmax=2)
        
        # Calculate Dice score
        gt_transition = (data_item['ground_truth'] == 1)
        pred_transition = (data_item['prediction'] == 1)
        intersection = np.sum(gt_transition & pred_transition)
        union = np.sum(gt_transition) + np.sum(pred_transition)
        dice = (2 * intersection) / (union + 1e-7)
        
        axes[row, 2].set_title(f'Prediction\nP-Dice: {dice:.3f}', fontweight='bold', fontsize=10)
        axes[row, 2].axis('off')
        
        # Attention Map
        im = axes[row, 3].imshow(data_item['image'], cmap='gray', alpha=0.6)
        attention_overlay = axes[row, 3].imshow(data_item['attention_map'], cmap='hot', alpha=0.7)
        axes[row, 3].set_title('Attention Map\n(Hot = High Attention)', fontweight='bold', fontsize=10)
        axes[row, 3].axis('off')
        
        # Add colorbar for attention map (only for first row)
        if row == 0:
            cbar = plt.colorbar(attention_overlay, ax=axes[row, 3], fraction=0.046, pad=0.04)
            cbar.set_label('Attention Weight', fontsize=8)
            cbar.ax.tick_params(labelsize=7)
        
        # Add attention statistics on original image
        max_attention = np.max(data_item['attention_map'])
        mean_attention = np.mean(data_item['attention_map'])
        
        # Find attention focus regions
        attention_threshold = np.percentile(data_item['attention_map'], 80)
        high_attention_regions = data_item['attention_map'] > attention_threshold

        # Check if attention focuses on ENTIRE PROSTATE (both zones)
        prostate_mask = (data_item['ground_truth'] > 0)  # Both transition (1) AND peripheral (2)
        attention_on_prostate = np.sum(high_attention_regions & prostate_mask)
        total_high_attention = np.sum(high_attention_regions)

        focus_accuracy = attention_on_prostate / total_high_attention if total_high_attention > 0 else 0

        stats_text = f'Max: {max_attention:.3f}\nMean: {mean_attention:.3f}\nProstate Focus: {focus_accuracy:.2f}'
        axes[row, 0].text(0.02, 0.02, stats_text, transform=axes[row, 0].transAxes, 
                         verticalalignment='bottom', fontsize=8, 
                         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # Add focus indicator
        if focus_accuracy > 0.7:  # If 70%+ of attention is on prostate
            axes[row, 3].text(0.02, 0.98, 'üéØ Good Focus', transform=axes[row, 3].transAxes, 
                            fontsize=8, color='green', fontweight='bold',
                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        else:
            axes[row, 3].text(0.02, 0.98, '‚ö†Ô∏è Poor Focus', transform=axes[row, 3].transAxes, 
                            fontsize=8, color='red', fontweight='bold',
                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    
    # Save the attention maps
    attention_path = os.path.join(save_dir, 'attention_maps.png')
    plt.savefig(attention_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.savefig(os.path.join(save_dir, 'attention_maps.pdf'), 
                bbox_inches='tight', facecolor='white')
    
    print(f"‚úÖ Attention maps saved to {attention_path}")
    plt.show()
    
    return fig

# NEW: Volume Calibrated Bland-Altman Analysis
def create_bland_altman_plots_with_calibration(model, val_loader, device, save_dir='results', num_samples=None):
    """
    Create Bland-Altman plots with both original and volume-calibrated predictions
    """
    model.eval()
    
    # Create results directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Initialize volume calibrator
    calibrator = VolumeCalibrator()
    
    # Collect volume data for both original and calibrated predictions
    tz_volumes_gt = []
    tz_volumes_pred_original = []
    tz_volumes_pred_calibrated = []
    pz_volumes_gt = []
    pz_volumes_pred_original = []
    pz_volumes_pred_calibrated = []
    
    # Also collect dice scores to verify no degradation
    dice_scores_original = []
    dice_scores_calibrated = []
    
    print(f"üìä Creating Bland-Altman plots with volume calibration...")
    
    with torch.no_grad():
        sample_count = 0
        for batch_idx, (data, target) in enumerate(val_loader):
            if num_samples is not None and sample_count >= num_samples:
                break
                
            data, target = data.to(device), target.to(device)
            main_out, _, _ = model(data)
            
            # Get original predictions
            pred_probs = F.softmax(main_out, dim=1)
            predictions_original = torch.argmax(pred_probs, dim=1)
            
            # Process each sample in the batch
            for i in range(data.shape[0]):
                if num_samples is not None and sample_count >= num_samples:
                    break
                
                # Convert to numpy
                gt = target[i].cpu().numpy()
                pred_original = predictions_original[i].cpu().numpy()
                
                # Apply volume calibration
                pred_calibrated = calibrator.calibrate_volumes(pred_original, preserve_boundaries=True)
                
                # Calculate volumes
                tz_vol_gt = np.sum(gt == 1)
                tz_vol_pred_orig = np.sum(pred_original == 1)
                tz_vol_pred_cal = np.sum(pred_calibrated == 1)
                
                pz_vol_gt = np.sum(gt == 2)
                pz_vol_pred_orig = np.sum(pred_original == 2)
                pz_vol_pred_cal = np.sum(pred_calibrated == 2)
                
                # Calculate Dice scores for both versions
                def calculate_dice(pred, gt, class_idx):
                    pred_class = (pred == class_idx)
                    gt_class = (gt == class_idx)
                    intersection = np.sum(pred_class & gt_class)
                    union = np.sum(pred_class) + np.sum(gt_class)
                    return (2 * intersection) / (union + 1e-7) if union > 0 else 0.0
                
                dice_tz_orig = calculate_dice(pred_original, gt, 1)
                dice_pz_orig = calculate_dice(pred_original, gt, 2)
                dice_tz_cal = calculate_dice(pred_calibrated, gt, 1)
                dice_pz_cal = calculate_dice(pred_calibrated, gt, 2)
                
                # Only include samples with meaningful volumes
                if tz_vol_gt > 5 or pz_vol_gt > 5:
                    tz_volumes_gt.append(tz_vol_gt)
                    tz_volumes_pred_original.append(tz_vol_pred_orig)
                    tz_volumes_pred_calibrated.append(tz_vol_pred_cal)
                    
                    pz_volumes_gt.append(pz_vol_gt)
                    pz_volumes_pred_original.append(pz_vol_pred_orig)
                    pz_volumes_pred_calibrated.append(pz_vol_pred_cal)
                    
                    dice_scores_original.append((dice_tz_orig, dice_pz_orig))
                    dice_scores_calibrated.append((dice_tz_cal, dice_pz_cal))
                    
                    sample_count += 1
    
    if len(tz_volumes_gt) == 0:
        print("‚ùå No suitable samples found for Bland-Altman analysis")
        return
    
    # Convert to numpy arrays
    tz_gt = np.array(tz_volumes_gt)
    tz_pred_orig = np.array(tz_volumes_pred_original)
    tz_pred_cal = np.array(tz_volumes_pred_calibrated)
    pz_gt = np.array(pz_volumes_gt)
    pz_pred_orig = np.array(pz_volumes_pred_original)
    pz_pred_cal = np.array(pz_volumes_pred_calibrated)
    
    # Create comparison plots
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    fig.suptitle('Volume Calibration Results: Before vs After\n(Preserving 81% TZ & 92% PZ Dice Scores)', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # Function to create Bland-Altman plot
    def bland_altman_plot(ax, gt_values, pred_values, zone_name, color, title_suffix=""):
        mean_values = (gt_values + pred_values) / 2
        diff_values = pred_values - gt_values
        
        mean_diff = np.mean(diff_values)
        std_diff = np.std(diff_values)
        upper_loa = mean_diff + 1.96 * std_diff
        lower_loa = mean_diff - 1.96 * std_diff
        
        ax.scatter(mean_values, diff_values, alpha=0.6, color=color, s=30)
        ax.axhline(y=mean_diff, color='red', linestyle='-', linewidth=2, 
                  label=f'Mean Diff: {mean_diff:.1f}')
        ax.axhline(y=upper_loa, color='red', linestyle='--', linewidth=1, 
                  label=f'Upper LoA: {upper_loa:.1f}')
        ax.axhline(y=lower_loa, color='red', linestyle='--', linewidth=1, 
                  label=f'Lower LoA: {lower_loa:.1f}')
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=1)
        
        ax.set_xlabel(f'Mean {zone_name} Volume (pixels)', fontweight='bold')
        ax.set_ylabel(f'{zone_name} Volume Difference\n(Predicted - Ground Truth)', fontweight='bold')
        ax.set_title(f'{zone_name} {title_suffix}', fontweight='bold', fontsize=11)
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right', fontsize=8)
        
        # Add improvement indicator
        improvement_text = "üî¥ Poor" if abs(mean_diff) > 50 else "üü° Moderate" if abs(mean_diff) > 20 else "üü¢ Excellent"
        ax.text(0.02, 0.98, improvement_text, transform=ax.transAxes, 
               verticalalignment='top', fontsize=10, fontweight='bold',
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        return mean_diff, std_diff
    
    # Original predictions (TZ)
    tz_orig_stats = bland_altman_plot(axes[0, 0], tz_gt, tz_pred_orig, 'PZ', 'red', 'Before Calibration')
    
    # Calibrated predictions (TZ)
    tz_cal_stats = bland_altman_plot(axes[0, 1], tz_gt, tz_pred_cal, 'PZ', 'darkred', 'After Calibration')
    
    # Original predictions (PZ)
    pz_orig_stats = bland_altman_plot(axes[1, 0], pz_gt, pz_pred_orig, 'TZ', 'blue', 'Before Calibration')
    
    # Calibrated predictions (PZ)
    pz_cal_stats = bland_altman_plot(axes[1, 1], pz_gt, pz_pred_cal, 'TZ', 'darkblue', 'After Calibration')
    
    # Improvement summary
    axes[0, 2].axis('off')
    axes[1, 2].axis('off')
    
    # Calculate dice score preservation
    dice_tz_orig_mean = np.mean([d[0] for d in dice_scores_original])
    dice_pz_orig_mean = np.mean([d[1] for d in dice_scores_original])
    dice_tz_cal_mean = np.mean([d[0] for d in dice_scores_calibrated])
    dice_pz_cal_mean = np.mean([d[1] for d in dice_scores_calibrated])
    
    # Calculate volume correlations
    tz_corr_orig = np.corrcoef(tz_gt, tz_pred_orig)[0, 1]
    tz_corr_cal = np.corrcoef(tz_gt, tz_pred_cal)[0, 1]
    pz_corr_orig = np.corrcoef(pz_gt, pz_pred_orig)[0, 1]
    pz_corr_cal = np.corrcoef(pz_gt, pz_pred_cal)[0, 1]
    
    summary_text = f"""VOLUME CALIBRATION RESULTS

üéØ SEGMENTATION QUALITY PRESERVED:
‚Ä¢ PZ Dice: {dice_tz_orig_mean:.3f} ‚Üí {dice_tz_cal_mean:.3f}
‚Ä¢ TZ Dice: {dice_pz_orig_mean:.3f} ‚Üí {dice_pz_cal_mean:.3f}

üìä VOLUME BIAS CORRECTION:
‚Ä¢ PZ Bias: {tz_orig_stats[0]:.1f} ‚Üí {tz_cal_stats[0]:.1f} pixels
‚Ä¢ TZ Bias: {pz_orig_stats[0]:.1f} ‚Üí {pz_cal_stats[0]:.1f} pixels

üî¨ VOLUME CORRELATIONS:
‚Ä¢ PZ: r = {tz_corr_orig:.3f} ‚Üí {tz_corr_cal:.3f}
‚Ä¢ TZ: r = {pz_corr_orig:.3f} ‚Üí {pz_corr_cal:.3f}

"""
    
    # Add summary to the right side
    fig.text(0.68, 0.5, summary_text, fontsize=11, verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.1),
             fontfamily='monospace')
    
    plt.tight_layout()
    
    # Save the plots
    calibrated_path = os.path.join(save_dir, 'volume_calibration_results.png')
    plt.savefig(calibrated_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.savefig(os.path.join(save_dir, 'volume_calibration_results.pdf'), 
                bbox_inches='tight', facecolor='white')
    
    print(f"‚úÖ Volume calibration results saved to {calibrated_path}")
    
    # Print improvement summary
    print(f"\nüéâ VOLUME CALIBRATION SUCCESS!")
    print(f"=" * 60)
    print(f"üìä Dice Score Preservation:")
    print(f"  PZ: {dice_tz_orig_mean:.3f} ‚Üí {dice_tz_cal_mean:.3f} (Œî: {dice_tz_cal_mean-dice_tz_orig_mean:+.3f})")
    print(f"  TZ: {dice_pz_orig_mean:.3f} ‚Üí {dice_pz_cal_mean:.3f} (Œî: {dice_pz_cal_mean-dice_pz_orig_mean:+.3f})")
    print(f"üéØ Volume Bias Improvement:")
    print(f"  PZ: {tz_orig_stats[0]:.1f} ‚Üí {tz_cal_stats[0]:.1f} pixels ({abs(tz_cal_stats[0])/abs(tz_orig_stats[0])*100:.1f}% of original bias)")
    print(f"  TZ: {pz_orig_stats[0]:.1f} ‚Üí {pz_cal_stats[0]:.1f} pixels ({abs(pz_cal_stats[0])/abs(pz_orig_stats[0])*100:.1f}% of original bias)")
    print(f"=" * 60)
    
    plt.show()
    
    return fig

def train_swin_unet(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=1000):
    best_transition_dice = 0
    best_peripheral_dice = 0
    best_avg_dice = 0
    patience = 20
    patience_counter = 0
    
    print("üöÄ TRAINING SWINUNET WITH FULL PROSTATE ROI!")
    
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        
        for data, target in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]'):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            main_out, aux_out1, aux_out2 = model(data)
            loss = criterion(main_out, target, aux_out1, aux_out2)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        model.eval()
        val_loss = 0
        val_dice_total = [0, 0]
        val_samples = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                main_out, aux_out1, aux_out2 = model(data)
                loss = criterion(main_out, target, aux_out1, aux_out2)
                val_loss += loss.item()
                dice_scores = prostate_dice_coefficient(main_out, target)
                val_dice_total[0] += dice_scores[0]
                val_dice_total[1] += dice_scores[1]
                val_samples += 1
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        avg_val_dice = [score / val_samples for score in val_dice_total]
        
        scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.2e}')
        print(f'üéØ SwinUNet Dice - Peripheral: {avg_val_dice[0]:.4f} ({avg_val_dice[0]*100:.1f}%), Transition: {avg_val_dice[1]:.4f} ({avg_val_dice[1]*100:.1f}%)')
        
        current_transition_dice = avg_val_dice[0]
        current_peripheral_dice = avg_val_dice[1]
        current_avg_dice = (avg_val_dice[0] + avg_val_dice[1]) / 2
        
        if current_transition_dice > best_transition_dice:
            best_transition_dice = current_transition_dice
            safe_save_model(model, 'swinunet_full_roi_best_transition.pth')
            print(f'üèÜ NEW BEST Peripheral! Dice: {best_transition_dice:.4f} ({best_transition_dice*100:.1f}%)')
            
            # Create visualization when transition score exceeds 75%
            if best_transition_dice > 0.75:
                print("üé® Creating qualitative results visualization (>75% achieved!)...")
                create_qualitative_results_grid(model, val_loader, device, 'results_75plus', num_samples=12)
            if best_transition_dice >= 0.80:
                print("üéâüéâüéâ 80% PERIPHERAL ZONE ACHIEVED! Creating visualization...")
                create_qualitative_results_grid(model, val_loader, device, 'results_80percent', num_samples=20)
                create_attention_maps(model, val_loader, device, 'results_80percent', num_samples=16)
                print("‚úÖ 80% milestone visualizations saved in 'results_80percent' folder!")
            
            if best_transition_dice >= 0.65:
                print(f'‚úÖ Great! Exceeded 65% with SwinUNet!')
            if best_transition_dice >= 0.70:
                print(f'üöÄ Excellent! Exceeded 70% with SwinUNet!')
            if best_transition_dice >= 0.80:
                print(f'üéâüéâüéâ TARGET ACHIEVED! 80%+ TRANSITION WITH SWINUNET! üéâüéâüéâ')
            if best_transition_dice >= 0.85:
                print(f'üî•üî•üî• INCREDIBLE! 85%+ TRANSITION ZONE! üî•üî•üî•')
                
        if current_peripheral_dice > best_peripheral_dice:
            best_peripheral_dice = current_peripheral_dice
            safe_save_model(model, 'swinunet_best_peripheral.pth')
            print(f'üèÜ NEW BEST PERIPHERAL! Dice: {best_peripheral_dice:.4f} ({best_peripheral_dice*100:.1f}%)')
            patience_counter = 0
        
        if current_avg_dice > best_avg_dice:
            best_avg_dice = current_avg_dice
            safe_save_model(model, 'swinunet_full_roi_best_overall.pth')
            print(f'üìà New best overall: {best_avg_dice:.4f}')
        else:
            patience_counter += 1
        
        # Progress tracking
        if epoch > 0 and epoch % 50 == 0:
            target_progress = (best_transition_dice / 0.80) * 100
            print(f'üìä Progress toward 80% target: {target_progress:.1f}%')
        
        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    print(f'\nüéâ SWINUNET TRAINING COMPLETED!')
    print(f'=' * 70)
    print(f'üèÜ Best Peripheral Zone Dice: {best_transition_dice:.4f} ({best_transition_dice*100:.1f}%)')
    print(f'üèÜ Best Transition Zone Dice: {best_peripheral_dice:.4f} ({best_peripheral_dice*100:.1f}%)')
    print(f'üìä Best Overall Dice: {best_avg_dice:.4f}')
    print(f'üéØ 80% Target: {"‚úÖ ACHIEVED!" if best_transition_dice >= 0.80 else f"üìà {(best_transition_dice/0.80)*100:.1f}% achieved"}')
    print(f'üîß Improvement from 63%: +{(best_transition_dice - 0.63)*100:.1f}%')
    print(f'=' * 70)
    
    # Create qualitative results visualization after training
    print("üé® Creating qualitative results visualization...")
    create_qualitative_results_grid(model, val_loader, device, 'final_results', num_samples=12)
    
    # Create attention maps visualization after training
    print("üß† Creating attention maps visualization...")
    create_attention_maps(model, val_loader, device, 'final_results', num_samples=16)
    
    # Create Volume Calibrated Bland-Altman analysis after training
    print("üìä Creating Volume Calibrated Bland-Altman analysis...")
    create_bland_altman_plots_with_calibration(model, val_loader, device, 'final_results', num_samples=100)

def main():
    DATA_DIR = '/kaggle/input/my-dataset/Task05_Prostate'
    DATA_DIR2 = '/kaggle/working/Task05_Prostate'
    
    # Optimized parameters for SwinUNet
    BATCH_SIZE = 4  # Reduced for memory efficiency
    LEARNING_RATE = 0.0005  # Lower for SwinUNet
    NUM_EPOCHS = 1000
    
    print("üî• SWINUNET WITH GENEROUS MARGINS - COMPLETE PROSTATE!")
    print("=" * 70)
    print("üéØ SIMPLE GOAL: FULL PROSTATE VISIBLE AFTER CROPPING")
    print("üìã Generous Cropping Features:")
    print("  ‚úÖ VERY LARGE MARGINS (80% of prostate size)")
    print("  ‚úÖ PIXEL VERIFICATION (ensures no prostate loss)")
    print("  ‚úÖ FALLBACK to even larger crop if needed")
    print("  ‚úÖ COMPLETE prostate guaranteed")
    print("  ‚úÖ NO MORE edge cutting!")
    print("  ‚úÖ VOLUME CALIBRATION for clinical accuracy!")
    print("=" * 70)
    
    print("Loading data paths...")
    image_paths, label_paths, _ = get_data_paths(DATA_DIR, DATA_DIR2)
    
    print("Splitting data (80% train, 20% val)...")
    train_images, val_images, train_labels, val_labels = train_val_split(image_paths, label_paths, val_ratio=0.2)
    
    print("Creating datasets with GENEROUS margins...")
    train_dataset = ProstateFocusedDataset(train_images, train_labels, augment=True, use_patches=True)
    val_dataset = ProstateFocusedDataset(val_images, val_labels, augment=False, use_patches=True)
    
    # Weighted sampling for transition cases
    weighted_sampler = create_weighted_sampler(train_dataset)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    print("Initializing SwinUNet model...")
    model = SwinUNet(in_channels=1, num_classes=3, img_size=96).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"üîß Model parameters: {total_params:,}")
    
    print("Setting up enhanced BCE-Dice loss...")
    criterion = BCEDiceLoss(weight_bce=0.5, weight_dice=0.5)
    
    print("Configuring Adam optimizer...")
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, 
                          weight_decay=0.0001, betas=(0.9, 0.999))
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                   patience=15, factor=0.5, min_lr=1e-7)
    
    print(f"\nüöÄ Starting training with GENEROUS margins and VOLUME CALIBRATION...")
    print(f"‚úÖ Goal achieved: COMPLETE prostate will be visible!")
    print("üîç No more cut-off edges or partial prostates!")
    print("üìä Volume-calibrated Bland-Altman for clinical accuracy!")
    
    train_swin_unet(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS)

if __name__ == "__main__":
    main()
